{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9e4c1-f27d-42f6-ac36-8988044d217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-13 20:04:16,999] Using an existing study with name 'lgb_smape_tuning' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna study ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc490fe9127a4dd89b6ee3892ebfbe65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-13 20:11:45,788] Trial 52 finished with value: 68.63368803595849 and parameters: {'boosting_type': 'dart', 'learning_rate': 0.01018545598783915, 'num_leaves': 203, 'max_depth': 7, 'feature_fraction': 0.6093661228916337, 'bagging_fraction': 0.8298913413858434, 'bagging_freq': 5, 'min_child_samples': 109, 'lambda_l1': 8.890341692616618, 'lambda_l2': 2.2067577117742885, 'min_split_gain': 0.8307586532788592}. Best is trial 52 with value: 68.63368803595849.\n",
      "Study completed.\n",
      "Best SMAPE: 68.633688\n",
      "Best params:\n",
      "  boosting_type: dart\n",
      "  learning_rate: 0.01018545598783915\n",
      "  num_leaves: 203\n",
      "  max_depth: 7\n",
      "  feature_fraction: 0.6093661228916337\n",
      "  bagging_fraction: 0.8298913413858434\n",
      "  bagging_freq: 5\n",
      "  min_child_samples: 109\n",
      "  lambda_l1: 8.890341692616618\n",
      "  lambda_l2: 2.2067577117742885\n",
      "  min_split_gain: 0.8307586532788592\n",
      "Retraining final model on full training data with best params...\n"
     ]
    }
   ],
   "source": [
    "# optuna_lgb_smape_tuning_fixed_callbacks.py\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_TRIALS = 1\n",
    "NUM_BOOST_ROUND = 1000\n",
    "EARLY_STOPPING_ROUNDS = 100\n",
    "\n",
    "# -------------------------\n",
    "# Load data + feature engineering\n",
    "# -------------------------\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "def extract_quantity(text):\n",
    "    value_match = re.search(r\"Value:\\s*([\\d.]+)\", str(text))\n",
    "    value = float(value_match.group(1)) if value_match else 1.0\n",
    "\n",
    "    pack_match = re.search(r\"pack of (\\d+)\", str(text).lower())\n",
    "    pack = int(pack_match.group(1)) if pack_match else 1\n",
    "\n",
    "    return value * pack\n",
    "\n",
    "train_df['quantity'] = train_df['catalog_content'].apply(extract_quantity).fillna(1)\n",
    "test_df['quantity'] = test_df['catalog_content'].apply(extract_quantity).fillna(1)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).replace('\\n',' ').lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['clean_text'] = train_df['catalog_content'].apply(clean_text).fillna(\"unknown\")\n",
    "test_df['clean_text'] = test_df['catalog_content'].apply(clean_text).fillna(\"unknown\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_text_train = vectorizer.fit_transform(train_df['clean_text'])\n",
    "X_text_test = vectorizer.transform(test_df['clean_text'])\n",
    "\n",
    "X_train = hstack([X_text_train, train_df[['quantity']].values.astype(np.float64)])\n",
    "X_test = hstack([X_text_test, test_df[['quantity']].values.astype(np.float64)])\n",
    "y_train = train_df['price'].values\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "# Train-validation split for tuning\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train_log, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "lgb_tr = lgb.Dataset(X_tr, label=y_tr)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_tr)\n",
    "\n",
    "# -------------------------\n",
    "# SMAPE functions\n",
    "# -------------------------\n",
    "def smape_np(y_true, y_pred):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) + 1e-9\n",
    "    return 100.0 * np.mean(2.0 * np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "def lgb_smape(preds, data):\n",
    "    labels = data.get_label()\n",
    "    y_true = np.expm1(labels)\n",
    "    y_pred = np.expm1(preds)\n",
    "    val = smape_np(y_true, y_pred)\n",
    "    return 'smape', val, False\n",
    "\n",
    "# -------------------------\n",
    "# Optuna objective (uses callbacks instead of early_stopping_rounds kw)\n",
    "# -------------------------\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 16, 512),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 16),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 10),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 200),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 10.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 10.0),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),\n",
    "        'verbosity': -1,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'deterministic': True,\n",
    "    }\n",
    "\n",
    "    # Use callbacks for early stopping & logging (compatible across LightGBM versions)\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(EARLY_STOPPING_ROUNDS),\n",
    "        lgb.log_evaluation(period=0)  # disable built-in logging here; set period>0 to enable\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        model = lgb.train(\n",
    "            param,\n",
    "            lgb_tr,\n",
    "            num_boost_round=NUM_BOOST_ROUND,\n",
    "            valid_sets=[lgb_tr, lgb_val],\n",
    "            valid_names=['train', 'val'],\n",
    "            feval=lgb_smape,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Trial failed:\", e)\n",
    "        return 1e6\n",
    "\n",
    "    best_iter = model.best_iteration if model.best_iteration is not None else NUM_BOOST_ROUND\n",
    "    val_preds_log = model.predict(X_val, num_iteration=best_iter)\n",
    "    val_preds = np.expm1(val_preds_log)\n",
    "    y_val_orig = np.expm1(y_val)\n",
    "\n",
    "    val_preds = np.clip(val_preds, 0.0, 1e9)\n",
    "    y_val_orig = np.clip(y_val_orig, 0.0, 1e9)\n",
    "\n",
    "    val_smape = smape_np(y_val_orig, val_preds)\n",
    "    trial.set_user_attr(\"best_iteration\", best_iter)\n",
    "    return val_smape\n",
    "\n",
    "# -------------------------\n",
    "# Run Optuna & final training (use callbacks here too)\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    storage_name = \"sqlite:///optuna_lgb_smape.db\"\n",
    "    study = optuna.create_study(\n",
    "        study_name=\"lgb_smape_tuning\",\n",
    "        direction=\"minimize\",\n",
    "        storage=storage_name,\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    print(\"Starting Optuna study ...\")\n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "    print(\"Study completed.\")\n",
    "    print(\"Best SMAPE: {:.6f}\".format(study.best_value))\n",
    "    print(\"Best params:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    best_params = study.best_params.copy()\n",
    "    best_params.update({\n",
    "        'objective': 'regression',\n",
    "        'verbosity': -1,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'deterministic': True\n",
    "    })\n",
    "\n",
    "    # Retrain on almost full data; keep small holdout for early stopping\n",
    "    X_tr_full, X_holdout, y_tr_full, y_holdout = train_test_split(X_train, y_train_log, test_size=0.05, random_state=RANDOM_SEED)\n",
    "    lgb_tr_full = lgb.Dataset(X_tr_full, label=y_tr_full)\n",
    "    lgb_holdout = lgb.Dataset(X_holdout, label=y_holdout, reference=lgb_tr_full)\n",
    "\n",
    "    # callbacks for final training: show log every 100 rounds\n",
    "    final_callbacks = [\n",
    "        lgb.early_stopping(EARLY_STOPPING_ROUNDS),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    "\n",
    "    print(\"Retraining final model on full training data with best params...\")\n",
    "    final_model = lgb.train(\n",
    "        best_params,\n",
    "        lgb_tr_full,\n",
    "        num_boost_round=NUM_BOOST_ROUND,\n",
    "        valid_sets=[lgb_tr_full, lgb_holdout],\n",
    "        valid_names=['train', 'holdout'],\n",
    "        feval=lgb_smape,\n",
    "        callbacks=final_callbacks,\n",
    "    )\n",
    "\n",
    "    best_iter_final = final_model.best_iteration if final_model.best_iteration is not None else NUM_BOOST_ROUND\n",
    "    print(\"Final model best iteration:\", best_iter_final)\n",
    "\n",
    "    # Evaluate on holdout\n",
    "    hold_preds_log = final_model.predict(X_holdout, num_iteration=best_iter_final)\n",
    "    hold_preds = np.expm1(hold_preds_log)\n",
    "    y_hold_orig = np.expm1(y_holdout)\n",
    "    hold_smape = smape_np(y_hold_orig, hold_preds)\n",
    "    hold_mae = mean_absolute_error(y_hold_orig, hold_preds)\n",
    "    print(f\"Holdout MAE: {hold_mae:.4f}\")\n",
    "    print(f\"Holdout SMAPE: {hold_smape:.4f}%\")\n",
    "\n",
    "    # Predict test set and save submission\n",
    "    test_preds_log = final_model.predict(X_test, num_iteration=best_iter_final)\n",
    "    test_preds = np.expm1(test_preds_log)\n",
    "    test_preds = np.clip(test_preds, 0.01, 1e9)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'sample_id': test_df['sample_id'],\n",
    "        'price': np.round(test_preds, 2)\n",
    "    })\n",
    "\n",
    "    out_name = 'test_out_optuna_fixed.csv'\n",
    "    submission.to_csv(out_name, index=False)\n",
    "    print(f\"âœ… Submission saved to {out_name}\")\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b871be6-04a0-4e0b-b3c3-42f300a70a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
