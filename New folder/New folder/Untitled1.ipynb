{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f4b653-bf33-4af8-9935-3858f8459fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.95, subsample=1.0 will be ignored. Current value: bagging_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.95, subsample=1.0 will be ignored. Current value: bagging_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.557147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 153261\n",
      "[LightGBM] [Info] Number of data points in the train set: 59247, number of used features: 603\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.95, subsample=1.0 will be ignored. Current value: bagging_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Start training from score 2.736635\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.95, subsample=1.0 will be ignored. Current value: bagging_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "==========================\n",
      "Final MAE: 15.1032\n",
      "Final SMAPE: 66.19%\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# LightGBM Regressor with SMAPE\n",
    "# =========================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================================================\n",
    "# SMAPE metric\n",
    "# =========================================================\n",
    "def smape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "# =========================================================\n",
    "# Load Data\n",
    "# =========================================================\n",
    "data = pd.read_csv(\"final_pca_whole_train_data.csv\")\n",
    "X = data.drop(columns=[\"price\", \"image_link\", \"sample_id\"])\n",
    "y = data[\"price\"].values\n",
    "\n",
    "# =========================================================\n",
    "# Train / Validation Split\n",
    "# =========================================================\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# Scale features\n",
    "# =========================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_val_scaled = scaler.transform(X_val_raw)\n",
    "\n",
    "# =========================================================\n",
    "# Log-transform target\n",
    "# =========================================================\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "# =========================================================\n",
    "# LightGBM model (scikit-learn API)\n",
    "# =========================================================\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=150,\n",
    "    num_leaves=32,\n",
    "    feature_fraction=0.9,\n",
    "    bagging_fraction=0.95,\n",
    "    bagging_freq=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# Fit with early stopping\n",
    "# =========================================================\n",
    "print(\"Training LightGBM...\")\n",
    "lgb_model.fit(\n",
    "    X_train_scaled, y_train_log,\n",
    "    eval_set=[(X_val_scaled, y_val_log)],\n",
    "    eval_metric=\"l2\",\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# Predict and evaluate\n",
    "# =========================================================\n",
    "y_pred_log = lgb_model.predict(X_val_scaled)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "smape_val = smape(y_val, y_pred)\n",
    "\n",
    "print(\"==========================\")\n",
    "print(f\"Final MAE: {mae:.4f}\")\n",
    "print(f\"Final SMAPE: {smape_val:.2f}%\")\n",
    "print(\"==========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9a84d25-26c9-4ba1-8069-5483bafa37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.17.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: tqdm in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
      "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
      "Downloading alembic-1.17.0-py3-none-any.whl (247 kB)\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "\n",
      "   ---------------------------------------- 0/4 [Mako]\n",
      "   ---------------------------------------- 0/4 [Mako]\n",
      "   ---------------------------------------- 0/4 [Mako]\n",
      "   ---------------------------------------- 0/4 [Mako]\n",
      "   ---------------------------------------- 0/4 [Mako]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ---------------------------------------- 4/4 [optuna]\n",
      "\n",
      "Successfully installed Mako-1.3.10 alembic-1.17.0 colorlog-6.9.0 optuna-4.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c56b976-698d-41d5-a8be-7e3d46ea7786",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LightGBMTunerCV.__init__() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 82\u001b[0m\n\u001b[0;32m     77\u001b[0m study_tuner \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m, study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgbm_tuner_v4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Run LightGBM TunerCV\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m tuner \u001b[38;5;241m=\u001b[39m \u001b[43mLightGBMTunerCV\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrkf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudy_tuner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# verbose_eval=False, # <-- THIS LINE WAS REMOVED\u001b[39;49;00m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_budget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸš€ Running Optuna LightGBM tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m tuner\u001b[38;5;241m.\u001b[39mrun()\n",
      "\u001b[1;31mTypeError\u001b[0m: LightGBMTunerCV.__init__() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# LightGBM Regressor with SMAPE + Optuna Tuning (v4 - Final)\n",
    "# =========================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from optuna.integration import LightGBMTunerCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================================================\n",
    "# SMAPE metric\n",
    "# =========================================================\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "# =========================================================\n",
    "# Load Data\n",
    "# =========================================================\n",
    "try:\n",
    "    data = pd.read_csv(\"final_pca_whole_train_data.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Data file not found. Creating a dummy dataframe for demonstration.\")\n",
    "    data = pd.DataFrame({\n",
    "        'price': np.random.lognormal(mean=8, sigma=1, size=500),\n",
    "        'feature1': np.random.rand(500) * 10,\n",
    "        'feature2': np.random.rand(500) * 5,\n",
    "        'feature3': np.random.randn(500),\n",
    "        'image_link': [f'link_{i}' for i in range(500)],\n",
    "        'sample_id': range(500)\n",
    "    })\n",
    "\n",
    "X = data.drop(columns=[\"price\", \"image_link\", \"sample_id\"])\n",
    "y = data[\"price\"].values\n",
    "\n",
    "# =========================================================\n",
    "# Preprocessing\n",
    "# =========================================================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "# =========================================================\n",
    "# LightGBM parameters for Optuna\n",
    "# =========================================================\n",
    "params = {\n",
    "    \"objective\": \"regression_l1\",\n",
    "    \"metric\": \"l1\",\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"seed\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# Prepare data + cross-validation\n",
    "# =========================================================\n",
    "dtrain = lgb.Dataset(X_scaled, label=y_log)\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "\n",
    "# =========================================================\n",
    "# Optuna Study\n",
    "# =========================================================\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study_tuner = optuna.create_study(direction='minimize', study_name='lgbm_tuner_v4')\n",
    "\n",
    "# =========================================================\n",
    "# Run LightGBM TunerCV\n",
    "# =========================================================\n",
    "tuner = LightGBMTunerCV(\n",
    "    params,\n",
    "    dtrain,\n",
    "    folds=rkf,\n",
    "    study=study_tuner,\n",
    "    # verbose_eval=False, # <-- THIS LINE WAS REMOVED\n",
    "    early_stopping_rounds=100,\n",
    "    time_budget=7200,\n",
    "    seed=42,\n",
    "    num_boost_round=10000,\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Running Optuna LightGBM tuning...\")\n",
    "tuner.run()\n",
    "\n",
    "# =========================================================\n",
    "# Best Parameters\n",
    "# =========================================================\n",
    "best_params = tuner.best_params\n",
    "print(\"\\n==========================\")\n",
    "print(\"ðŸ† Best parameters found by Optuna:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"==========================\")\n",
    "\n",
    "# =========================================================\n",
    "# Final Training using best parameters\n",
    "# =========================================================\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    dtrain,\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# Evaluate with a more reliable metric\n",
    "# =========================================================\n",
    "best_cv_score = study_tuner.best_value\n",
    "print(f\"ðŸ“Š Best Cross-Validated MAE (on log-price): {best_cv_score:.6f}\")\n",
    "print(\"==========================\")\n",
    "\n",
    "y_pred_log_insample = final_model.predict(X_scaled)\n",
    "y_pred_insample = np.expm1(y_pred_log_insample)\n",
    "smape_insample = smape(y, y_pred_insample)\n",
    "print(f\"âš ï¸ In-Sample SMAPE (for reference only): {smape_insample:.2f}%\")\n",
    "print(\"==========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e199ca21-597c-44f7-b58b-3cec854f689a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optuna-integration[lightgbm]\n",
      "  Downloading optuna_integration-4.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: optuna in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna-integration[lightgbm]) (4.5.0)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna-integration[lightgbm]) (4.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna-integration[lightgbm]) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from lightgbm->optuna-integration[lightgbm]) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from lightgbm->optuna-integration[lightgbm]) (1.15.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna->optuna-integration[lightgbm]) (1.17.0)\n",
      "Requirement already satisfied: colorlog in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna->optuna-integration[lightgbm]) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna->optuna-integration[lightgbm]) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna->optuna-integration[lightgbm]) (2.0.41)\n",
      "Requirement already satisfied: tqdm in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna->optuna-integration[lightgbm]) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna->optuna-integration[lightgbm]) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from alembic>=1.5.0->optuna->optuna-integration[lightgbm]) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from alembic>=1.5.0->optuna->optuna-integration[lightgbm]) (4.14.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[lightgbm]) (3.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from colorlog->optuna->optuna-integration[lightgbm]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration[lightgbm]) (2.1.5)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->optuna-integration[lightgbm])\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\satya\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->optuna-integration[lightgbm]) (3.5.0)\n",
      "Downloading optuna_integration-4.5.0-py3-none-any.whl (99 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Installing collected packages: joblib, optuna-integration\n",
      "\n",
      "  Attempting uninstall: joblib\n",
      "\n",
      "    Found existing installation: joblib 1.1.1\n",
      "\n",
      "    Uninstalling joblib-1.1.1:\n",
      "\n",
      "      Successfully uninstalled joblib-1.1.1\n",
      "\n",
      "   ---------------------------------------- 0/2 [joblib]\n",
      "   ---------------------------------------- 0/2 [joblib]\n",
      "   ---------------------------------------- 0/2 [joblib]\n",
      "   ---------------------------------------- 0/2 [joblib]\n",
      "   ---------------------------------------- 0/2 [joblib]\n",
      "   ---------------------------------------- 0/2 [joblib]\n",
      "   ---------------------------------------- 0/2 [joblib]\n",
      "   ---------------------------------------- 0/2 [joblib]\n",
      "   -------------------- ------------------- 1/2 [optuna-integration]\n",
      "   -------------------- ------------------- 1/2 [optuna-integration]\n",
      "   -------------------- ------------------- 1/2 [optuna-integration]\n",
      "   -------------------- ------------------- 1/2 [optuna-integration]\n",
      "   -------------------- ------------------- 1/2 [optuna-integration]\n",
      "   ---------------------------------------- 2/2 [optuna-integration]\n",
      "\n",
      "Successfully installed joblib-1.5.2 optuna-integration-4.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.5.2 which is incompatible.\n",
      "pandas-profiling 3.2.0 requires visions[type_image_path]==0.7.4, but you have visions 0.8.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna-integration[lightgbm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68b30b32-44e5-4e40-9938-c1125b6215d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (8000, 20)\n",
      "================================================================================\n",
      "\n",
      "1. GRID SEARCH\n",
      "--------------------------------------------------------------------------------\n",
      "New best score: 0.9250\n",
      "  Params: leaves=31, depth=5, lr=0.01, min_leaf=20\n",
      "New best score: 0.9590\n",
      "  Params: leaves=31, depth=5, lr=0.05, min_leaf=20\n",
      "New best score: 0.9640\n",
      "  Params: leaves=31, depth=5, lr=0.1, min_leaf=20\n",
      "New best score: 0.9650\n",
      "  Params: leaves=31, depth=5, lr=0.1, min_leaf=50\n",
      "New best score: 0.9655\n",
      "  Params: leaves=31, depth=10, lr=0.1, min_leaf=20\n",
      "New best score: 0.9700\n",
      "  Params: leaves=31, depth=10, lr=0.1, min_leaf=50\n",
      "New best score: 0.9720\n",
      "  Params: leaves=50, depth=15, lr=0.1, min_leaf=20\n",
      "\n",
      "Best Grid Search Score: 0.9720\n",
      "Best Params: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 50, 'max_depth': 15, 'learning_rate': 0.1, 'min_data_in_leaf': 20, 'verbose': -1}\n",
      "\n",
      "\n",
      "2. RANDOM SEARCH\n",
      "--------------------------------------------------------------------------------\n",
      "Trial 1: New best score: 0.9140\n",
      "Trial 2: New best score: 0.9535\n",
      "Trial 14: New best score: 0.9650\n",
      "\n",
      "Best Random Search Score: 0.9650\n",
      "Dataset shape: (8000, 20)\n",
      "Target range (original): 0 - 1\n",
      "================================================================================\n",
      "\n",
      "1. RANDOM SEARCH\n",
      "--------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[370]\tvalid_0's rmse: 0.209902\n",
      "Trial 1: New best RMSE: 0.3118\n",
      "  RÂ² Score: 0.6111\n",
      "  MAE: 0.2415\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 0.147022\n",
      "Trial 2: New best RMSE: 0.2195\n",
      "  RÂ² Score: 0.8073\n",
      "  MAE: 0.1461\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 0.172699\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[220]\tvalid_0's rmse: 0.222548\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 0.229002\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's rmse: 0.196592\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[150]\tvalid_0's rmse: 0.176266\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[356]\tvalid_0's rmse: 0.220247\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[235]\tvalid_0's rmse: 0.206636\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 0.196629\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmse: 0.208751\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 0.186889\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[347]\tvalid_0's rmse: 0.221776\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[354]\tvalid_0's rmse: 0.146235\n",
      "Trial 14: New best RMSE: 0.2175\n",
      "  RÂ² Score: 0.8108\n",
      "  MAE: 0.1487\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's rmse: 0.205635\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[215]\tvalid_0's rmse: 0.213269\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 0.187868\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[229]\tvalid_0's rmse: 0.200878\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[443]\tvalid_0's rmse: 0.228359\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's rmse: 0.18126\n",
      "\n",
      "Best Random Search RMSE: 0.2175\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Create sample dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15,\n",
    "                           n_redundant=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Dataset shape:\", X_train.shape)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 1: GRID SEARCH\n",
    "# ============================================================================\n",
    "print(\"\\n1. GRID SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'min_data_in_leaf': [20, 50]\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for num_leaves in param_grid['num_leaves']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for min_leaf in param_grid['min_data_in_leaf']:\n",
    "                params = {\n",
    "                    'objective': 'binary',\n",
    "                    'metric': 'binary_logloss',\n",
    "                    'num_leaves': num_leaves,\n",
    "                    'max_depth': max_depth,\n",
    "                    'learning_rate': lr,\n",
    "                    'min_data_in_leaf': min_leaf,\n",
    "                    'verbose': -1\n",
    "                }\n",
    "                \n",
    "                # Train with early stopping\n",
    "                train_data = lgb.Dataset(X_train, label=y_train)\n",
    "                model = lgb.train(\n",
    "                    params,\n",
    "                    train_data,\n",
    "                    num_boost_round=500,\n",
    "                    valid_sets=[train_data],\n",
    "                    callbacks=[lgb.early_stopping(50)]\n",
    "                )\n",
    "                \n",
    "                y_pred = model.predict(X_test) > 0.5\n",
    "                score = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = params.copy()\n",
    "                    print(f\"New best score: {score:.4f}\")\n",
    "                    print(f\"  Params: leaves={num_leaves}, depth={max_depth}, lr={lr}, min_leaf={min_leaf}\")\n",
    "\n",
    "print(f\"\\nBest Grid Search Score: {best_score:.4f}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 2: RANDOM SEARCH\n",
    "# ============================================================================\n",
    "print(\"\\n\\n2. RANDOM SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "np.random.seed(42)\n",
    "best_score_random = 0\n",
    "best_params_random = {}\n",
    "\n",
    "for trial in range(20):  # 20 random trials\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'num_leaves': np.random.randint(20, 150),\n",
    "        'max_depth': np.random.randint(3, 20),\n",
    "        'learning_rate': np.random.uniform(0.01, 0.2),\n",
    "        'min_data_in_leaf': np.random.randint(10, 100),\n",
    "        'lambda_l1': np.random.uniform(0, 50),\n",
    "        'lambda_l2': np.random.uniform(0, 50),\n",
    "        'feature_fraction': np.random.uniform(0.5, 1.0),\n",
    "        'bagging_fraction': np.random.uniform(0.5, 1.0),\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[train_data],\n",
    "        callbacks=[lgb.early_stopping(50)]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_test) > 0.5\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    if score > best_score_random:\n",
    "        best_score_random = score\n",
    "        best_params_random = params.copy()\n",
    "        print(f\"Trial {trial+1}: New best score: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Random Search Score: {best_score_random:.4f}\")\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# =========================================================\n",
    "# Preprocessing\n",
    "# =========================================================\n",
    "# X = data.drop(columns=[\"price\", \"image_link\", \"sample_id\"])\n",
    "# y = data[\"price\"].values\n",
    "\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_val_scaled = scaler.transform(X_val_raw)\n",
    "\n",
    "# Log-transform target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "print(\"Dataset shape:\", X_train_scaled.shape)\n",
    "print(\"Target range (original):\", y_train.min(), \"-\", y_train.max())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 1: RANDOM SEARCH (REGRESSION)\n",
    "# ============================================================================\n",
    "print(\"\\n1. RANDOM SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "np.random.seed(42)\n",
    "best_score_random = float('inf')\n",
    "best_params_random = {}\n",
    "best_model_random = None\n",
    "\n",
    "for trial in range(20):  # 20 random trials\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'num_leaves': np.random.randint(20, 150),\n",
    "        'max_depth': np.random.randint(3, 20),\n",
    "        'learning_rate': np.random.uniform(0.01, 0.2),\n",
    "        'min_data_in_leaf': np.random.randint(10, 100),\n",
    "        'lambda_l1': np.random.uniform(0, 50),\n",
    "        'lambda_l2': np.random.uniform(0, 50),\n",
    "        'feature_fraction': np.random.uniform(0.5, 1.0),\n",
    "        'bagging_fraction': np.random.uniform(0.5, 1.0),\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_scaled, label=y_train_log)\n",
    "    val_data = lgb.Dataset(X_val_scaled, label=y_val_log, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(-1)]\n",
    "    )\n",
    "    \n",
    "    # Predict and inverse transform\n",
    "    y_val_pred_log = model.predict(X_val_scaled)\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    \n",
    "    if rmse < best_score_random:\n",
    "        best_score_random = rmse\n",
    "        best_params_random = params.copy()\n",
    "        best_model_random = model\n",
    "        print(f\"Trial {trial+1}: New best RMSE: {rmse:.4f}\")\n",
    "        print(f\"  RÂ² Score: {r2_score(y_val, y_val_pred):.4f}\")\n",
    "        print(f\"  MAE: {mean_absolute_error(y_val, y_val_pred):.4f}\")\n",
    "\n",
    "print(f\"\\nBest Random Search RMSE: {best_score_random:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a26a54-68e3-447b-b63d-a49ed18d4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=250,\n",
    "    num_leaves=32,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.85,\n",
    "    bagging_freq=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "Final MAE: 15.0662\n",
    "Final SMAPE: 65.98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b50a9-28ca-4b7d-ad04-6f77ced99229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (8000, 20)\n",
      "================================================================================\n",
      "\n",
      "1. GRID SEARCH\n",
      "--------------------------------------------------------------------------------\n",
      "New best score: 0.9250\n",
      "  Params: leaves=31, depth=5, lr=0.01, min_leaf=20\n",
      "New best score: 0.9590\n",
      "  Params: leaves=31, depth=5, lr=0.05, min_leaf=20\n",
      "New best score: 0.9640\n",
      "  Params: leaves=31, depth=5, lr=0.1, min_leaf=20\n",
      "New best score: 0.9650\n",
      "  Params: leaves=31, depth=5, lr=0.1, min_leaf=50\n",
      "New best score: 0.9655\n",
      "  Params: leaves=31, depth=10, lr=0.1, min_leaf=20\n",
      "New best score: 0.9700\n",
      "  Params: leaves=31, depth=10, lr=0.1, min_leaf=50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "\n",
    "print(\"Dataset shape:\", X_train.shape)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 1: GRID SEARCH\n",
    "# ============================================================================\n",
    "print(\"\\n1. GRID SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'min_data_in_leaf': [20, 50]\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for num_leaves in param_grid['num_leaves']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for min_leaf in param_grid['min_data_in_leaf']:\n",
    "                params = {\n",
    "                    'objective': 'binary',\n",
    "                    'metric': 'binary_logloss',\n",
    "                    'num_leaves': num_leaves,\n",
    "                    'max_depth': max_depth,\n",
    "                    'learning_rate': lr,\n",
    "                    'min_data_in_leaf': min_leaf,\n",
    "                    'verbose': -1\n",
    "                }\n",
    "                \n",
    "                # Train with early stopping\n",
    "                train_data = lgb.Dataset(X_train, label=y_train)\n",
    "                model = lgb.train(\n",
    "                    params,\n",
    "                    train_data,\n",
    "                    num_boost_round=500,\n",
    "                    valid_sets=[train_data],\n",
    "                    callbacks=[lgb.early_stopping(50)]\n",
    "                )\n",
    "                \n",
    "                y_pred = model.predict(X_test) > 0.5\n",
    "                score = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = params.copy()\n",
    "                    print(f\"New best score: {score:.4f}\")\n",
    "                    print(f\"  Params: leaves={num_leaves}, depth={max_depth}, lr={lr}, min_leaf={min_leaf}\")\n",
    "\n",
    "print(f\"\\nBest Grid Search Score: {best_score:.4f}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 2: RANDOM SEARCH\n",
    "# ============================================================================\n",
    "print(\"\\n\\n2. RANDOM SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "np.random.seed(42)\n",
    "best_score_random = 0\n",
    "best_params_random = {}\n",
    "\n",
    "for trial in range(20):  # 20 random trials\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'num_leaves': np.random.randint(20, 150),\n",
    "        'max_depth': np.random.randint(3, 20),\n",
    "        'learning_rate': np.random.uniform(0.01, 0.2),\n",
    "        'min_data_in_leaf': np.random.randint(10, 100),\n",
    "        'lambda_l1': np.random.uniform(0, 50),\n",
    "        'lambda_l2': np.random.uniform(0, 50),\n",
    "        'feature_fraction': np.random.uniform(0.5, 1.0),\n",
    "        'bagging_fraction': np.random.uniform(0.5, 1.0),\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[train_data],\n",
    "        callbacks=[lgb.early_stopping(50)]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_test) > 0.5\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    if score > best_score_random:\n",
    "        best_score_random = score\n",
    "        best_params_random = params.copy()\n",
    "        print(f\"Trial {trial+1}: New best score: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Random Search Score: {best_score_random:.4f}\")\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# =========================================================\n",
    "# Preprocessing\n",
    "# =========================================================\n",
    "# X = data.drop(columns=[\"price\", \"image_link\", \"sample_id\"])\n",
    "# y = data[\"price\"].values\n",
    "data = pd.read_csv(\"final_pca_whole_train_data.csv\")\n",
    "X = data.drop(columns=[\"price\", \"image_link\", \"sample_id\"])\n",
    "y = data[\"price\"].values\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_val_scaled = scaler.transform(X_val_raw)\n",
    "\n",
    "# Log-transform target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "print(\"Dataset shape:\", X_train_scaled.shape)\n",
    "print(\"Target range (original):\", y_train.min(), \"-\", y_train.max())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 1: RANDOM SEARCH (REGRESSION)\n",
    "# ============================================================================\n",
    "print(\"\\n1. RANDOM SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "np.random.seed(42)\n",
    "best_score_random = float('inf')\n",
    "best_params_random = {}\n",
    "best_model_random = None\n",
    "\n",
    "for trial in range(20):  # 20 random trials\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'num_leaves': np.random.randint(20, 150),\n",
    "        'max_depth': np.random.randint(3, 20),\n",
    "        'learning_rate': np.random.uniform(0.01, 0.2),\n",
    "        'min_data_in_leaf': np.random.randint(10, 100),\n",
    "        'lambda_l1': np.random.uniform(0, 50),\n",
    "        'lambda_l2': np.random.uniform(0, 50),\n",
    "        'feature_fraction': np.random.uniform(0.5, 1.0),\n",
    "        'bagging_fraction': np.random.uniform(0.5, 1.0),\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_scaled, label=y_train_log)\n",
    "    val_data = lgb.Dataset(X_val_scaled, label=y_val_log, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(-1)]\n",
    "    )\n",
    "    \n",
    "    # Predict and inverse transform\n",
    "    y_val_pred_log = model.predict(X_val_scaled)\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    \n",
    "    if rmse < best_score_random:\n",
    "        best_score_random = rmse\n",
    "        best_params_random = params.copy()\n",
    "        best_model_random = model\n",
    "        print(f\"Trial {trial+1}: New best RMSE: {rmse:.4f}\")\n",
    "        print(f\"  RÂ² Score: {r2_score(y_val, y_val_pred):.4f}\")\n",
    "        print(f\"  MAE: {mean_absolute_error(y_val, y_val_pred):.4f}\")\n",
    "\n",
    "print(f\"\\nBest Random Search RMSE: {best_score_random:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 2: BAYESIAN OPTIMIZATION WITH OPTUNA\n",
    "# ============================================================================\n",
    "print(\"\\n\\n2. BAYESIAN OPTIMIZATION (OPTUNA)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "\n",
    "# =========================================================\n",
    "# Preprocessing\n",
    "# =========================================================\n",
    "# X = data.drop(columns=[\"price\", \"image_link\", \"sample_id\"])\n",
    "# y = data[\"price\"].values\n",
    "\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_val_scaled = scaler.transform(X_val_raw)\n",
    "\n",
    "# Log-transform target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "print(\"Dataset shape:\", X_train_scaled.shape)\n",
    "print(\"Target range (original):\", y_train.min(), \"-\", y_train.max())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# RANDOM SEARCH\n",
    "# ============================================================================\n",
    "print(\"\\nRANDOM SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "np.random.seed(42)\n",
    "best_score_random = float('inf')\n",
    "best_params_random = {}\n",
    "best_model_random = None\n",
    "\n",
    "for trial in range(20):  # 20 random trials\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'num_leaves': np.random.randint(20, 150),\n",
    "        'max_depth': np.random.randint(3, 20),\n",
    "        'learning_rate': np.random.uniform(0.01, 0.2),\n",
    "        'min_data_in_leaf': np.random.randint(10, 100),\n",
    "        'lambda_l1': np.random.uniform(0, 50),\n",
    "        'lambda_l2': np.random.uniform(0, 50),\n",
    "        'feature_fraction': np.random.uniform(0.5, 1.0),\n",
    "        'bagging_fraction': np.random.uniform(0.5, 1.0),\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_scaled, label=y_train_log)\n",
    "    val_data = lgb.Dataset(X_val_scaled, label=y_val_log, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(-1)]\n",
    "    )\n",
    "    \n",
    "    # Predict and inverse transform\n",
    "    y_val_pred_log = model.predict(X_val_scaled)\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    \n",
    "    if rmse < best_score_random:\n",
    "        best_score_random = rmse\n",
    "        best_params_random = params.copy()\n",
    "        best_model_random = model\n",
    "        print(f\"Trial {trial+1}: New best RMSE: {rmse:.4f}\")\n",
    "        print(f\"  RÂ² Score: {r2_score(y_val, y_val_pred):.4f}\")\n",
    "        print(f\"  MAE: {mean_absolute_error(y_val, y_val_pred):.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL FROM RANDOM SEARCH\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Best RMSE: {best_score_random:.4f}\")\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for key, value in best_params_random.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATE BEST MODEL\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL EVALUATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "y_val_pred_log = best_model_random.predict(X_val_scaled)\n",
    "y_val_pred = np.expm1(y_val_pred_log)\n",
    "\n",
    "final_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "final_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "final_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"RÂ² Score: {final_r2:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AND USE THE BEST MODEL\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SAVING BEST MODEL\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Save model\n",
    "pickle.dump(best_model_random, open('best_model.pkl', 'wb'))\n",
    "print(\"âœ“ Model saved to 'best_model.pkl'\")\n",
    "\n",
    "# Save scaler\n",
    "pickle.dump(scaler, open('scaler.pkl', 'wb'))\n",
    "print(\"âœ“ Scaler saved to 'scaler.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c8c82-e0fa-495f-ac6f-7602a35bc7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83541d35-afbe-4b87-ba67-0b2d8c1cf725",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 603 features, but StandardScaler is expecting 20 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 25\u001b[0m\n\u001b[0;32m     10\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m drop_cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m test_data\u001b[38;5;241m.\u001b[39mcolumns], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Align test columns to match training columns\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Scale test data (same scaler as training)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Predict on test data\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[0;32m     30\u001b[0m y_test_pred_log \u001b[38;5;241m=\u001b[39m best_model_random\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\preprocessing\\_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2965\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 603 features, but StandardScaler is expecting 20 features as input."
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Load test data\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "test_data = pd.read_csv(\"final_pca_whole_test_data.csv\")  # ðŸ‘ˆ Replace with your test dataset path\n",
    "sample_id = test_data['sample_id']\n",
    "\n",
    "# Drop unnecessary columns that are not part of training\n",
    "drop_cols = ['sample_id','image_link']  # ensure you donâ€™t have this in test\n",
    "test_data = test_data.drop(columns=[c for c in drop_cols if c in test_data.columns], errors='ignore')\n",
    "\n",
    "# =========================================================\n",
    "# Align test columns to match training columns\n",
    "# =========================================================\n",
    "# missing_cols = set(X_train_raw.columns) - set(test_data.columns)\n",
    "# for col in missing_cols:\n",
    "#     test_data[col] = 0  # fill missing columns with 0\n",
    "\n",
    "# # Ensure the same column order as training\n",
    "# test_data = test_data[X_train_raw.columns]\n",
    "\n",
    "# =========================================================\n",
    "# Scale test data (same scaler as training)\n",
    "# =========================================================\n",
    "X_test_scaled = scaler.transform(test_data)\n",
    "\n",
    "# =========================================================\n",
    "# Predict on test data\n",
    "# =========================================================\n",
    "y_test_pred_log = best_model_random.predict(X_test_scaled)\n",
    "y_test_pred = np.expm1(y_test_pred_log)  # reverse log-transform\n",
    "\n",
    "# =========================================================\n",
    "# Create submission file\n",
    "# =========================================================\n",
    "# If your test file has a sample_id column, keep it\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': sample_id,\n",
    "    'price': y_test_pred\n",
    "    })\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv(\"submissionn.csv\", index=False)\n",
    "print(\"âœ… Predictions saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244410f-54e2-4504-a90f-035b4e5079ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e8013-6b03-49d1-9836-ef8328118046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5a3d1-72d6-4cbd-9cd1-047a24cd2c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76afa6-b016-4843-9555-175ae2359569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e851e-d71e-4f8d-8c82-d910cb568368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
